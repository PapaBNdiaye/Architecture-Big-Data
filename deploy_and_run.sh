#!/bin/bash

# Script d'automatisation pour le dÃ©ploiement et l'exÃ©cution du traitement batch
# Architecture Lambda - Couche Batch

set -e  # ArrÃªter le script en cas d'erreur

echo "ğŸš€ DÃ©marrage de l'automatisation du traitement batch..."
echo "=================================================="

# Configuration de l'API Key
echo "ğŸ“ Configuration de l'API Key Visual Crossing..."
export VISUAL_CROSSING_API_KEY="votre_clÃ©_api"

# DÃ©marrage des services Docker
echo "ğŸ³ DÃ©marrage des services Docker..."
docker-compose up -d

# Attendre que les services soient prÃªts
echo "â³ Attente du dÃ©marrage des services (30 secondes)..."
sleep 30

# Installation des dÃ©pendances Python dans les conteneurs Spark
echo "ğŸ“¦ Installation des dÃ©pendances Python..."
echo "  - Installation dans spark-master..."
docker exec spark-master pip install -r //opt/spark/requirements_batch.txt

echo "  - Installation dans spark-worker..."
docker exec spark-worker pip install -r //opt/spark/requirements_batch.txt

# Configuration des permissions HDFS
echo "ğŸ” Configuration des permissions HDFS..."
echo "  - Suppression des anciens rÃ©pertoires weather/raw..."
docker exec -it hdfs-namenode bash -c "hdfs dfs -rm -r -f /user/spark/weather/raw" || echo "    (Aucun ancien rÃ©pertoire Ã  supprimer)"

echo "  - CrÃ©ation du rÃ©pertoire weather/raw..."
docker exec -it hdfs-namenode bash -c "hdfs dfs -mkdir -p /user/spark/weather/raw"

echo "  - Attribution des permissions Ã  spark:spark..."
docker exec -it hdfs-namenode bash -c "hdfs dfs -chown -R spark:spark /user/spark"

# Attendre que HDFS soit complÃ¨tement initialisÃ©
echo "â³ Attente de l'initialisation complÃ¨te de HDFS (15 secondes)..."
sleep 15

# ExÃ©cution du traitement batch
echo "âš¡ ExÃ©cution du traitement batch..."
docker exec -it spark-master spark-submit \
  --master spark://spark-master:7077 \
  --py-files //opt/spark/project/config/cities_config.py \
  //opt/spark/project/batch_processing_local.py

# Attendre la fin du traitement
echo "â³ Attente de la fin du traitement (10 secondes)..."
sleep 10

# VÃ©rification du contenu HDFS
echo "ğŸ” VÃ©rification du contenu HDFS..."
echo "  - Liste des rÃ©pertoires dans /user/spark/weather/raw/..."
docker exec hdfs-namenode bash -c "hdfs dfs -ls /user/spark/weather/raw/"

# Trouver le rÃ©pertoire le plus rÃ©cent
echo "  - Recherche du rÃ©pertoire le plus rÃ©cent..."
LATEST_DIR=$(docker exec hdfs-namenode bash -c "hdfs dfs -ls /user/spark/weather/raw/ | grep "2025" | tail -1 | awk '{print $8}' | xargs basename")
echo "  - RÃ©pertoire trouvÃ©: $LATEST_DIR"

# ExÃ©cution du script de comptage avec le bon chemin
echo "ğŸ“Š ExÃ©cution du script de comptage des donnÃ©es..."
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  //opt/spark/project/count_data.py

echo ""
echo "âœ… Traitement terminÃ© avec succÃ¨s!"
echo "=================================================="
echo "ğŸŒ Interfaces disponibles:"
echo "  - HDFS Web UI: http://localhost:9871"
echo "  - Spark Master UI: http://localhost:9090"
echo "  - Airflow UI: http://localhost:8080"
echo "  - Kafka Control Center: http://localhost:9021"
echo ""
echo "ğŸ“ DonnÃ©es stockÃ©es dans HDFS: /user/spark/weather/raw/$LATEST_DIR"
echo "ğŸ” Pour voir les donnÃ©es: docker exec hdfs-namenode bash -c 'hdfs dfs -ls /user/spark/weather/raw/$LATEST_DIR'"
