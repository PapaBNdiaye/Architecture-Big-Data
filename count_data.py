#!/usr/bin/env python3
"""
Script pour compter les lignes dans les donnÃ©es HDFS
"""

from pyspark.sql import SparkSession
import os
import subprocess

def find_latest_hdfs_directory():
    """Trouve le rÃ©pertoire le plus rÃ©cent dans HDFS"""
    try:
        # Utiliser hadoop fs pour lister les rÃ©pertoires
        cmd = ["hadoop", "fs", "-ls", "/user/spark/weather/raw/"]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        # Parser la sortie pour trouver les rÃ©pertoires avec des timestamps
        lines = result.stdout.strip().split('\n')
        directories = []
        
        for line in lines:
            if '2025' in line and 'drwx' in line:
                # Extraire le nom du rÃ©pertoire (dernier Ã©lÃ©ment du chemin)
                path = line.split()[-1]
                dir_name = os.path.basename(path)
                directories.append(dir_name)
        
        if directories:
            # Trier par nom (qui contient le timestamp) et prendre le plus rÃ©cent
            directories.sort(reverse=True)
            return directories[0]
        else:
            return None
            
    except Exception as e:
        print(f"âš ï¸ Erreur lors de la recherche du rÃ©pertoire HDFS: {e}")
        return None

def main():
    spark = SparkSession.builder \
        .appName("CountWeatherData") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    # Trouver le rÃ©pertoire le plus rÃ©cent
    latest_dir = find_latest_hdfs_directory()
    
    if latest_dir is None:
        print("âŒ Aucun rÃ©pertoire de donnÃ©es trouvÃ© dans HDFS")
        print("ğŸ’¡ Assurez-vous que le traitement batch a Ã©tÃ© exÃ©cutÃ©")
        spark.stop()
        return
    
    hdfs_path = f"hdfs://hdfs-namenode:8020/user/spark/weather/raw/{latest_dir}"
    print(f"ğŸ“ Lecture des donnÃ©es depuis: {hdfs_path}")
    
    try:
        # Lire les donnÃ©es Parquet depuis HDFS
        df = spark.read.parquet(hdfs_path)
    except Exception as e:
        print(f"âŒ Erreur lors de la lecture des donnÃ©es: {e}")
        spark.stop()
        return
    
    # Compter les lignes
    count = df.count()
    columns = len(df.columns)
    
    print(f"ğŸ“Š Nombre de lignes: {count}")
    print(f"ğŸ“Š Nombre de colonnes: {columns}")
    print(f"ğŸ“Š Colonnes: {df.columns}")
    
    # Afficher quelques exemples
    print("\nğŸ” PremiÃ¨res lignes:")
    df.show(5)
    
    # Statistiques sur les donnÃ©es
    print("\nğŸ“ˆ Statistiques:")
    df.describe().show()
    
    spark.stop()

if __name__ == "__main__":
    main()
