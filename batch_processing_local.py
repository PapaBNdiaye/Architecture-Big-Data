#!/usr/bin/env python3
"""
Traitement batch FULL SPARK (version simplifiÃ©e) :
- Spark distribue les appels Ã  l'API Visual Crossing
- Chaque worker tÃ©lÃ©charge les donnÃ©es pour une ville
- Les donnÃ©es brutes sont sauvegardÃ©es dans HDFS
"""

import requests
import time
from datetime import datetime
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col

# Import config
import sys, os
sys.path.append(os.path.join(os.path.dirname(__file__), "config"))
from cities_config import CITIES, API_CONFIG, get_api_params

API_KEY = os.getenv("VISUAL_CROSSING_API_KEY", "VOTRE_API_KEY")
BASE_URL = API_CONFIG['base_url']

# === Fonction exÃ©cutÃ©e par chaque worker ===
def fetch_city_weather(city, start_date="2025-01-01", end_date="2025-01-10"):
    max_retries = 3
    base_delay = 5
    
    for attempt in range(max_retries):
        try:
            url = f"{BASE_URL}/{city}/{start_date}/{end_date}"
            params = get_api_params()
            params["key"] = API_KEY

            print(f"ğŸ“¡ Fetching {city} â†’ {url} (tentative {attempt + 1}/{max_retries})")  # Debug
            
            # DÃ©lai progressif entre les tentatives
            if attempt > 0:
                delay = base_delay * (2 ** attempt)  # Backoff exponentiel
                print(f"â³ Attente de {delay} secondes avant la tentative {attempt + 1}...")
                time.sleep(delay)
            else:
                time.sleep(2)  # DÃ©lai normal pour la premiÃ¨re tentative
            
            response = requests.get(url, params=params, timeout=30)
            print(f"ğŸ”‘ Status {response.status_code} for {city}")  # Debug

            if response.status_code == 429:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  Limite de taux atteinte pour {city}, nouvelle tentative dans {base_delay * (2 ** attempt)} secondes...")
                    continue
                else:
                    print(f"âŒ Limite de taux persistante pour {city} aprÃ¨s {max_retries} tentatives")
                    return []

            response.raise_for_status()
            data = response.json()
            days = data.get("days", [])

            print(f"ğŸ“Š {city} â†’ {len(days)} jours reÃ§us")  # Debug

            results = []
            for d in days:
                # RÃ©cupÃ©rer toutes les colonnes disponibles depuis l'API
                result = {
                    "city": city,
                    "date": d.get("datetime"),
                    "address": d.get("address"),
                    "latitude": d.get("latitude"),
                    "longitude": d.get("longitude"),
                    "temp": d.get("temp"),
                    "tempmax": d.get("tempmax"),
                    "tempmin": d.get("tempmin"),
                    "humidity": d.get("humidity"),
                    "dew": d.get("dew"),
                    "precip": d.get("precip"),
                    "preciptype": d.get("preciptype"),
                    "pressure": d.get("pressure"),
                    "windspeed": d.get("windspeed"),
                    "windgust": d.get("windgust"),
                    "winddir": d.get("winddir"),
                    "cloudcover": d.get("cloudcover"),
                    "visibility": d.get("visibility")
                }
                results.append(result)
            return results

        except Exception as e:
            print(f"âŒ Erreur pour {city} (tentative {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                return []
            continue
    
    return []


def main():
    # VÃ©rification de l'API key
    if API_KEY == "VOTRE_API_KEY":
        print("âŒ ERREUR: API key non configurÃ©e! DÃ©finissez la variable d'environnement VISUAL_CROSSING_API_KEY")
        return
    
    print(f"ğŸ”‘ API Key configurÃ©e: {API_KEY[:8]}...")
    
    spark = SparkSession.builder \
        .appName("WeatherBatchProcessingFullSparkRaw") \
        .config("spark.hadoop.fs.defaultFS", "hdfs://hdfs-namenode:8020") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # === Liste des villes depuis config ===
    cities = CITIES

    # === Distribution des appels API ===
    print(f"ğŸ™ï¸  Traitement de {len(cities)} villes: {cities}")
    rdd = spark.sparkContext.parallelize(cities, len(cities))
    results_rdd = rdd.flatMap(lambda city: fetch_city_weather(city))
    
    # Debug: compter les rÃ©sultats avant crÃ©ation du DataFrame
    results_count = results_rdd.count()
    print(f"ğŸ“Š Nombre total de rÃ©sultats rÃ©cupÃ©rÃ©s: {results_count}")
    
    if results_count == 0:
        print("âŒ Aucune donnÃ©e rÃ©cupÃ©rÃ©e! VÃ©rifiez votre API key et la connectivitÃ©.")
        spark.stop()
        return
    
    # Collecter quelques exemples pour debug
    sample_results = results_rdd.take(3)
    print(f"ğŸ” Exemples de donnÃ©es: {sample_results}")

    # DÃ©finir le schÃ©ma explicitement pour Ã©viter les erreurs d'infÃ©rence
    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType
    
    schema = StructType([
        StructField("city", StringType(), True),
        StructField("date", StringType(), True),
        StructField("address", StringType(), True),
        StructField("latitude", DoubleType(), True),
        StructField("longitude", DoubleType(), True),
        StructField("temp", DoubleType(), True),
        StructField("tempmax", DoubleType(), True),
        StructField("tempmin", DoubleType(), True),
        StructField("humidity", DoubleType(), True),
        StructField("dew", DoubleType(), True),
        StructField("precip", DoubleType(), True),
        StructField("preciptype", StringType(), True),
        StructField("pressure", DoubleType(), True),
        StructField("windspeed", DoubleType(), True),
        StructField("windgust", DoubleType(), True),
        StructField("winddir", DoubleType(), True),
        StructField("cloudcover", DoubleType(), True),
        StructField("visibility", DoubleType(), True)
    ])
    
    weather_df = spark.createDataFrame(results_rdd.map(lambda x: Row(**x)), schema)

    # Convertir la colonne date en type date
    weather_df = weather_df.withColumn("date", col("date").cast("date"))

    # === Sauvegarde uniquement des donnÃ©es brutes ===
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    raw_output = f"hdfs://hdfs-namenode:8020/user/spark/weather/raw/{timestamp}"
    weather_df.write.mode("overwrite").parquet(raw_output)

    print(f"âœ… DonnÃ©es brutes sauvegardÃ©es dans {raw_output}")

    spark.stop()

if __name__ == "__main__":
    main()
