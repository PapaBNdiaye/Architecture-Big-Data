# ğŸš€ Instructions de dÃ©marrage

## PrÃ©requis
- Docker et Docker Compose installÃ©s
- Python 3.8+ installÃ©
- ClÃ© API Visual Crossing

## DÃ©marrage rapide

### 1. Cloner le projet
```bash
git clone <votre-repo>
cd Architecture-Big-Data
```

### 2. CrÃ©er un environnement virtuel Python
```bash
# CrÃ©er l'environnement virtuel
python -m venv venv

# Activer l'environnement virtuel
# Sur Windows :
venv\Scripts\activate
# Sur Linux/Mac :
source venv/bin/activate

# VÃ©rifier que l'environnement est activÃ©
which python  # Doit pointer vers venv/bin/python (ou venv\Scripts\python sur Windows)
```

### 3. Installer les dÃ©pendances Python (optionnel pour dÃ©veloppement local)
```bash
# Installer les dÃ©pendances pour le dÃ©veloppement local
pip install -r requirements_local.txt

# Ou pour le traitement batch uniquement
pip install -r requirements_batch.txt
```

### 4. Configurer la clÃ© API
**Option A - Variable d'environnement :**
```bash
export VISUAL_CROSSING_API_KEY=votre_cle_api_ici
```

**Option B - Fichier .env :**
```bash
echo "VISUAL_CROSSING_API_KEY=votre_cle_api_ici" > .env
```

### 5. DÃ©marrer l'infrastructure
```bash
docker-compose up -d
```

### 6. VÃ©rifier le statut
```bash
docker ps
```

## ğŸ”‘ Obtenir une clÃ© API Visual Crossing

1. Allez sur : https://www.visualcrossing.com/weather-api
2. CrÃ©ez un compte gratuit
3. Obtenez votre clÃ© API (1000 requÃªtes/jour gratuites)

## ğŸŒ Interfaces disponibles

- **HDFS Web UI** : http://localhost:9871
- **Spark UI** : http://localhost:9090
- **Airflow** : http://localhost:8080 (admin/admin)
- **Kafka Control Center** : http://localhost:9021

## ğŸ§ª Tester le pipeline

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --py-files //opt/spark/project/config/cities_config.py \
  //opt/spark/project/batch_processing_local.py
```

## ğŸ› ï¸ DÃ©pannage

### ProblÃ¨me : "zookeeper is unhealthy"
- VÃ©rifiez que la variable `VISUAL_CROSSING_API_KEY` est dÃ©finie
- RedÃ©marrez : `docker-compose down && docker-compose up -d`

### ProblÃ¨me : "ModuleNotFoundError: No module named 'requests'"
```bash
docker exec spark-master pip install -r /opt/spark/requirements_batch.txt
```

### ProblÃ¨me : "Permission denied: user=spark, access=WRITE"
```bash
# CrÃ©er le rÃ©pertoire pour l'utilisateur spark
docker exec -it hdfs-namenode bash -c "hdfs dfs -mkdir -p /user/spark/weather/raw"

# Donner les permissions Ã  l'utilisateur spark
docker exec -it hdfs-namenode bash -c "hdfs dfs -chown -R spark:spark /user/spark"

# VÃ©rifier les permissions
docker exec -it hdfs-namenode bash -c "hdfs dfs -ls -la /user/"
```

## ğŸ“Š VÃ©rifier les donnÃ©es

```bash
docker exec hdfs-namenode bash -c "hdfs dfs -ls /user/spark/weather/raw/"
```

## ğŸ—‘ï¸ Nettoyer HDFS (recommencer Ã  zÃ©ro)

### Supprimer toutes les donnÃ©es raw
```bash
# Supprimer le rÃ©pertoire raw complet
docker exec hdfs-namenode bash -c "hdfs dfs -rm -r /user/spark/weather/raw"

# VÃ©rifier que c'est supprimÃ©
docker exec hdfs-namenode bash -c "hdfs dfs -ls /user/spark/weather/"
```

### Supprimer tout le rÃ©pertoire weather (optionnel)
```bash
# Supprimer tout le rÃ©pertoire weather
docker exec hdfs-namenode bash -c "hdfs dfs -rm -r /user/spark/weather"

# VÃ©rifier
docker exec hdfs-namenode bash -c "hdfs dfs -ls /user/spark/"
```

### Nettoyage complet (si nÃ©cessaire)
```bash
# Supprimer tout le rÃ©pertoire utilisateur spark
docker exec hdfs-namenode bash -c "hdfs dfs -rm -r /user/spark"

# RecrÃ©er la structure
docker exec hdfs-namenode bash -c "hdfs dfs -mkdir -p /user/spark/weather/raw"
docker exec hdfs-namenode bash -c "hdfs dfs -chown -R spark:spark /user/spark"
```

## ğŸ”„ Gestion de l'environnement virtuel

### DÃ©sactiver l'environnement virtuel
```bash
deactivate
```

### RÃ©activer l'environnement virtuel
```bash
# Sur Windows :
venv\Scripts\activate
# Sur Linux/Mac :
source venv/bin/activate
```

### Supprimer l'environnement virtuel (si nÃ©cessaire)
```bash
# DÃ©sactiver d'abord
deactivate

# Supprimer le dossier
rm -rf venv  # Linux/Mac
rmdir /s venv  # Windows
```
