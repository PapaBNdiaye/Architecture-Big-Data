#!/usr/bin/env python3
"""
Script d'agrÃ©gation et d'analyse des donnÃ©es mÃ©tÃ©o
RÃ©pond aux questions d'analyse mÃ©tÃ©o avancÃ©es
ExÃ©cutÃ© dans l'environnement Docker Spark
"""

from datetime import datetime, timedelta
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml.stat import Correlation
import numpy as np

# Import config depuis le rÃ©pertoire config montÃ© dans Docker
import sys, os
sys.path.append("/opt/spark/project/config")
from cities_config import CITIES, API_CONFIG, get_api_params

class WeatherAnalyzer:
    def __init__(self):
        self.spark = SparkSession.builder \
            .appName("WeatherAnalysisAdvanced") \
            .config("spark.hadoop.fs.defaultFS", "hdfs://hdfs-namenode:8020") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        
    def load_weather_data(self, hdfs_path=None):
        """Charge les donnÃ©es mÃ©tÃ©o depuis HDFS"""
        if hdfs_path is None:
            # Trouver le rÃ©pertoire le plus rÃ©cent
            hdfs_base = "hdfs://hdfs-namenode:8020/user/spark/weather/raw"
            # Pour la dÃ©mo, on utilise un chemin fixe
            hdfs_path = f"{hdfs_base}/20250925_133714"
        
        print(f"ğŸ“Š Chargement des donnÃ©es depuis {hdfs_path}")
        
        try:
            df = self.spark.read.parquet(hdfs_path)
            print(f"âœ… {df.count()} enregistrements chargÃ©s")
            return df
        except Exception as e:
            print(f"âŒ Erreur lors du chargement: {e}")
            return None
    
    def analyze_extremes_and_variability(self, df):
        """1. Analyse des extrÃªmes et variabilitÃ© mÃ©tÃ©o"""
        print("\nğŸŒ¡ï¸  === ANALYSE DES EXTRÃŠMES ET VARIABILITÃ‰ ===")
        
        # TempÃ©ratures extrÃªmes par ville
        temp_extremes = df.groupBy("city") \
            .agg(
                avg("temp").alias("temp_moyenne"),
                max("tempmax").alias("temp_max_absolue"),
                min("tempmin").alias("temp_min_absolue"),
                stddev("temp").alias("variabilite_temp"),
                max("precip").alias("precip_max"),
                max("windspeed").alias("vent_max"),
                avg("humidity").alias("humidite_moyenne")
            ) \
            .orderBy(desc("temp_max_absolue"))
        
        print("ğŸ† Villes avec les tempÃ©ratures les plus extrÃªmes:")
        temp_extremes.show()
        
        # Jours avec conditions extrÃªmes
        extreme_days = df.filter(
            (col("tempmax") > 35) |  # Vague de chaleur
            (col("tempmin") < -5) |  # Vague de froid
            (col("precip") > 20) |   # Fortes prÃ©cipitations
            (col("windspeed") > 15)  # Vent fort
        ).orderBy(desc("tempmax"))
        
        print("âš ï¸  Jours avec conditions mÃ©tÃ©o extrÃªmes:")
        extreme_days.select("city", "date", "tempmax", "tempmin", "precip", "windspeed").show(20)
        
        return temp_extremes, extreme_days
    
    def analyze_city_comparisons(self, df):
        """2. Comparaisons et benchmarks entre villes"""
        print("\nğŸ™ï¸  === COMPARAISONS ENTRE VILLES ===")
        
        # Statistiques comparatives par ville
        city_stats = df.groupBy("city") \
            .agg(
                avg("temp").alias("temp_moyenne"),
                avg("precip").alias("precip_moyenne"),
                avg("windspeed").alias("vent_moyen"),
                avg("cloudcover").alias("nuages_moyen"),
                avg("visibility").alias("visibilite_moyenne"),
                count("*").alias("nb_jours")
            ) \
            .orderBy(desc("temp_moyenne"))
        
        print("ğŸ“Š Comparaison des villes (moyennes):")
        city_stats.show()
        
        # Classements
        rankings = {}
        
        # Ville la plus ensoleillÃ©e (moins de nuages)
        sunniest = df.groupBy("city") \
            .agg(avg("cloudcover").alias("nuages_moyen")) \
            .orderBy("nuages_moyen") \
            .first()
        rankings["ensoleillee"] = sunniest["city"]
        
        # Ville la plus pluvieuse
        rainiest = df.groupBy("city") \
            .agg(avg("precip").alias("precip_moyenne")) \
            .orderBy(desc("precip_moyenne")) \
            .first()
        rankings["pluvieuse"] = rainiest["city"]
        
        # Ville la plus venteuse
        windiest = df.groupBy("city") \
            .agg(avg("windspeed").alias("vent_moyen")) \
            .orderBy(desc("vent_moyen")) \
            .first()
        rankings["venteuse"] = windiest["city"]
        
        print("ğŸ† Classements des villes:")
        print(f"â˜€ï¸  Plus ensoleillÃ©e: {rankings['ensoleillee']}")
        print(f"ğŸŒ§ï¸  Plus pluvieuse: {rankings['pluvieuse']}")
        print(f"ğŸ’¨ Plus venteuse: {rankings['venteuse']}")
        
        return city_stats, rankings
    
    def analyze_correlations_and_patterns(self, df):
        """3. CorrÃ©lations et patterns mÃ©tÃ©o"""
        print("\nğŸ”— === CORRÃ‰LATIONS ET PATTERNS ===")
        
        # PrÃ©paration des donnÃ©es pour l'analyse de corrÃ©lation
        correlation_data = df.select(
            "temp", "humidity", "precip", "windspeed", 
            "pressure", "cloudcover", "visibility"
        ).na.drop()
        
        # Conversion en vecteur pour l'analyse de corrÃ©lation
        assembler = VectorAssembler(
            inputCols=["temp", "humidity", "precip", "windspeed", "pressure", "cloudcover", "visibility"],
            outputCol="features"
        )
        
        vector_data = assembler.transform(correlation_data)
        
        # Calcul de la matrice de corrÃ©lation
        correlation_matrix = Correlation.corr(vector_data, "features").head()[0]
        
        print("ğŸ“ˆ Matrice de corrÃ©lation:")
        features = ["temp", "humidity", "precip", "windspeed", "pressure", "cloudcover", "visibility"]
        corr_array = correlation_matrix.toArray()
        
        for i, feature1 in enumerate(features):
            for j, feature2 in enumerate(features):
                if i < j:  # Ã‰viter les doublons
                    corr_value = corr_array[i][j]
                    print(f"{feature1} â†” {feature2}: {corr_value:.3f}")
        
        # Clustering mÃ©tÃ©o par ville
        print("\nğŸ¯ Clustering des villes par conditions mÃ©tÃ©o:")
        
        city_weather = df.groupBy("city") \
            .agg(
                avg("temp").alias("temp_moyen"),
                avg("humidity").alias("humidity_moyen"),
                avg("precip").alias("precip_moyen"),
                avg("windspeed").alias("windspeed_moyen")
            )
        
        # Assemblage pour clustering
        cluster_assembler = VectorAssembler(
            inputCols=["temp_moyen", "humidity_moyen", "precip_moyen", "windspeed_moyen"],
            outputCol="weather_features"
        )
        
        cluster_data = cluster_assembler.transform(city_weather)
        
        # K-means clustering
        kmeans = KMeans(k=2, seed=1, featuresCol="weather_features")
        model = kmeans.fit(cluster_data)
        predictions = model.transform(cluster_data)
        
        print("Clusters mÃ©tÃ©o:")
        predictions.select("city", "prediction").show()
        
        return correlation_matrix, predictions
    
    def analyze_risks_and_projections(self, df):
        """4. Analyse des risques et projections"""
        print("\nâš ï¸  === ANALYSE DES RISQUES ET PROJECTIONS ===")
        
        # Identification des pÃ©riodes Ã  risque
        risk_periods = df.filter(
            (col("tempmax") > 35) |  # Risque de chaleur
            (col("tempmin") < -5) |  # Risque de froid
            (col("precip") > 15) |   # Risque de prÃ©cipitations
            (col("windspeed") > 12)  # Risque de vent fort
        ).orderBy(desc("date"))
        
        print("ğŸš¨ PÃ©riodes Ã  risque identifiÃ©es:")
        risk_periods.select("city", "date", "tempmax", "tempmin", "precip", "windspeed").show(10)
        
        # Analyse des tendances par mois
        monthly_trends = df.withColumn("month", month("date")) \
            .groupBy("city", "month") \
            .agg(
                avg("temp").alias("temp_moyenne"),
                avg("precip").alias("precip_moyenne"),
                avg("windspeed").alias("vent_moyen")
            ) \
            .orderBy("city", "month")
        
        print("ğŸ“… Tendances mensuelles:")
        monthly_trends.show(20)
        
        return risk_periods, monthly_trends
    
    def analyze_energy_agriculture(self, df):
        """5. Analyses Ã©nergie et agriculture"""
        print("\nğŸŒ± === ANALYSES Ã‰NERGIE ET AGRICULTURE ===")
        
        # Calcul des degrÃ©s-jours (approximation)
        df_with_degree_days = df.withColumn(
            "heating_degree_days", 
            when(col("temp") < 18, 18 - col("temp")).otherwise(0)
        ).withColumn(
            "cooling_degree_days",
            when(col("temp") > 24, col("temp") - 24).otherwise(0)
        )
        
        # Analyse par ville
        energy_analysis = df_with_degree_days.groupBy("city") \
            .agg(
                avg("heating_degree_days").alias("degres_chauffage_moyen"),
                avg("cooling_degree_days").alias("degres_climatisation_moyen"),
                avg("temp").alias("temp_moyenne"),
                avg("cloudcover").alias("nuages_moyen")
            )
        
        print("âš¡ Analyse Ã©nergÃ©tique par ville:")
        energy_analysis.show()
        
        # Potentiel solaire (approximation basÃ©e sur la couverture nuageuse)
        solar_potential = df.groupBy("city") \
            .agg(
                avg("cloudcover").alias("nuages_moyen"),
                avg("visibility").alias("visibilite_moyenne")
            ) \
            .withColumn("potentiel_solaire", 100 - col("nuages_moyen"))
        
        print("â˜€ï¸  Potentiel solaire par ville:")
        solar_potential.show()
        
        return energy_analysis, solar_potential
    
    def generate_summary_report(self, results):
        """GÃ©nÃ¨re un rapport de synthÃ¨se"""
        print("\nğŸ“‹ === RAPPORT DE SYNTHÃˆSE ===")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde des rÃ©sultats dans HDFS
        output_path = f"hdfs://hdfs-namenode:8020/user/spark/weather/analysis/{timestamp}"
        
        for name, data in results.items():
            if data is not None:
                # Si c'est un tuple, sauvegarder chaque Ã©lÃ©ment
                if isinstance(data, tuple):
                    for i, df in enumerate(data):
                        if hasattr(df, 'write'):  # VÃ©rifier que c'est un DataFrame
                            df.write.mode("overwrite").parquet(f"{output_path}/{name}_{i}")
                            print(f"âœ… {name}_{i} sauvegardÃ© dans {output_path}/{name}_{i}")
                else:
                    # Si c'est un DataFrame simple
                    if hasattr(data, 'write'):
                        data.write.mode("overwrite").parquet(f"{output_path}/{name}")
                        print(f"âœ… {name} sauvegardÃ© dans {output_path}/{name}")
        
        print(f"\nğŸ¯ Toutes les analyses sont disponibles dans: {output_path}")
        
        return output_path
    
    def run_complete_analysis(self, hdfs_path=None):
        """ExÃ©cute l'analyse complÃ¨te"""
        print("ğŸš€ DÃ©marrage de l'analyse mÃ©tÃ©o complÃ¨te...")
        
        # Chargement des donnÃ©es
        df = self.load_weather_data(hdfs_path)
        if df is None:
            return None
        
        # ExÃ©cution de toutes les analyses
        results = {}
        
        try:
            results["extremes"] = self.analyze_extremes_and_variability(df)
            results["comparisons"] = self.analyze_city_comparisons(df)
            results["correlations"] = self.analyze_correlations_and_patterns(df)
            results["risks"] = self.analyze_risks_and_projections(df)
            results["energy"] = self.analyze_energy_agriculture(df)
            
            # GÃ©nÃ©ration du rapport
            output_path = self.generate_summary_report(results)
            
            print("\nâœ… Analyse complÃ¨te terminÃ©e!")
            return output_path
            
        except Exception as e:
            print(f"âŒ Erreur lors de l'analyse: {e}")
            return None
        
        finally:
            self.spark.stop()

def main():
    print("ğŸŒ¤ï¸  ANALYSE MÃ‰TÃ‰O BIG DATA")
    print("=" * 50)
    
    analyzer = WeatherAnalyzer()
    output_path = analyzer.run_complete_analysis()
    
    if output_path:
        print(f"\nğŸ‰ Analyse terminÃ©e! RÃ©sultats dans: {output_path}")
        print("\nğŸ’¡ Pour utiliser le frontend web:")
        print("   python weather_frontend.py")
    else:
        print("\nâŒ Ã‰chec de l'analyse")

if __name__ == "__main__":
    main()
