# Architecture Lambda - Couche Batch

Ce document d√©crit l'impl√©mentation de la couche batch de l'architecture Lambda pour le traitement des donn√©es m√©t√©o.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   API Visual    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Batch Processor ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ      HDFS      ‚îÇ
‚îÇ   Crossing      ‚îÇ    ‚îÇ   (Spark)       ‚îÇ    ‚îÇ   (Stockage)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ     Airflow     ‚îÇ
                       ‚îÇ  (Orchestration) ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Composants

### 1. HDFS (Hadoop Distributed File System)
- **Namenode**: `hdfs-namenode:9870` (Interface web)
- **Datanode**: `hdfs-datanode` (Stockage des donn√©es)
- **Volumes persistants**: `hadoop_namenode`, `hadoop_datanode`

### 2. Traitement Batch
- **Script principal**: `batch_processing.py`
- **Framework**: Apache Spark
- **Format de stockage**: Parquet (optimis√© pour les requ√™tes analytiques)

### 3. Orchestration
- **DAG Airflow**: `dags/weather_batch_dag.py`
- **Planification**: Ex√©cution quotidienne
- **Monitoring**: Interface Airflow sur `localhost:8080`

## üöÄ D√©ploiement

### Pr√©requis
```bash
# Variables d'environnement requises
export VISUAL_CROSSING_API_KEY="votre_cl√©_api"
```

### 1. D√©marrage des services
```bash
# D√©marrage de tous les services
docker-compose up -d

# V√©rification du statut
docker-compose ps
```

### 2. Configuration de l'environnement
```bash
# Configuration compl√®te
python setup_batch_environment.py --full-setup

# V√©rification des services
python setup_batch_environment.py --check-services

# Configuration HDFS
python setup_batch_environment.py --setup-hdfs
```

### 3. Test du traitement batch
```bash
# Test manuel
python batch_processing.py

# Via Airflow (interface web)
# http://localhost:8080
```

## üìä Donn√©es Trait√©es

### Villes trait√©es
- **Villes fran√ßaises principales**: Paris, Lyon, Marseille, Toulouse, Nice, Nantes, Strasbourg, Montpellier, Bordeaux, Lille
- **Traitement parall√®le**: 3 villes simultan√©ment pour optimiser les performances
- **Configuration flexible**: Facilement modifiable dans `config/cities_config.py`

### Variables m√©t√©o r√©cup√©r√©es
- **Temp√©rature**: `temp`, `tempmax`, `tempmin`
- **Humidit√©**: `humidity`, `dew`
- **Pr√©cipitations**: `precip`, `preciptype`, `snow`, `snowdepth`
- **Pression**: `pressure`
- **Vent**: `windspeed`, `windgust`, `winddir`
- **Visibilit√©**: `visibility`, `cloudcover`
- **Rayonnement**: `uvindex`, `solarradiation`, `solarenergy`
- **Localisation**: `latitude`, `longitude`, `address`

### Agr√©gations calcul√©es
- **Temp√©rature**: Moyenne, maximum, minimum par jour
- **Humidit√©**: Moyenne, maximum, minimum par jour
- **Pression**: Moyenne, maximum, minimum par jour
- **Vent**: Moyenne et maximum par jour
- **Pr√©cipitations**: Total et maximum par jour
- **Rayonnement**: Total d'√©nergie solaire par jour

## üìÅ Structure HDFS

```
/weather_data/
‚îú‚îÄ‚îÄ raw/                    # Donn√©es brutes de l'API
‚îÇ   ‚îú‚îÄ‚îÄ Paris_FR/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250101_143022/
‚îÇ   ‚îú‚îÄ‚îÄ Lyon_FR/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250101_143022/
‚îÇ   ‚îú‚îÄ‚îÄ Marseille_FR/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250101_143022/
‚îÇ   ‚îî‚îÄ‚îÄ ... (autres villes)
‚îú‚îÄ‚îÄ batch/                  # Donn√©es agr√©g√©es
‚îÇ   ‚îú‚îÄ‚îÄ Paris_FR/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250101_143022/
‚îÇ   ‚îú‚îÄ‚îÄ Lyon_FR/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250101_143022/
‚îÇ   ‚îú‚îÄ‚îÄ Marseille_FR/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 20250101_143022/
‚îÇ   ‚îî‚îÄ‚îÄ ... (autres villes)
‚îî‚îÄ‚îÄ processed/              # Donn√©es finales (futur)
```

## üîß Configuration

### Variables Airflow
Configurez ces variables dans l'interface Airflow (`localhost:8080`):

```python
VISUAL_CROSSING_API_KEY = "votre_cl√©_api"
HDFS_URL = "hdfs://hdfs-namenode:9000"
SPARK_MASTER_URL = "spark://spark-master:7077"
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
```

### Param√®tres du DAG
```python
# Ex√©cution quotidienne √† 6h00
schedule_interval = '@daily'

# Retry en cas d'√©chec
retries = 2
retry_delay = timedelta(minutes=5)
```

## üìà Monitoring

### Interfaces de monitoring
- **Airflow**: http://localhost:8080
- **HDFS Namenode**: http://localhost:9870
- **Spark Master**: http://localhost:9090
- **Kafka Control Center**: http://localhost:9021

### Logs
```bash
# Logs Airflow
docker-compose logs scheduler
docker-compose logs webserver

# Logs HDFS
docker-compose logs hdfs-namenode
docker-compose logs hdfs-datanode

# Logs Spark
docker-compose logs spark-master
docker-compose logs spark-worker
```

## üß™ Tests

### Test unitaire du processeur
```bash
python -c "
from batch_processing import WeatherBatchProcessor
processor = WeatherBatchProcessor('test_key')
print('Processeur initialis√© avec succ√®s')
"
```

### Test de connectivit√© HDFS
```bash
# V√©rification du cluster HDFS
docker exec hdfs-namenode hdfs dfsadmin -report

# Liste des fichiers
docker exec hdfs-namenode hdfs dfs -ls /weather_data
```

### Test multi-villes
```bash
# Test du traitement de plusieurs villes
python test_multi_cities.py
```

### Test de l'API Visual Crossing
```bash
# Test de l'endpoint API pour une ville
curl "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/Paris,FR/2025-01-01/2025-01-01?key=VOTRE_API_KEY&include=days&elements=temp,humidity&contentType=csv"
```

## üîç D√©pannage

### Probl√®mes courants

1. **HDFS non accessible**
   ```bash
   # V√©rifier le statut des conteneurs
   docker-compose ps
   
   # Red√©marrer HDFS
   docker-compose restart hdfs-namenode hdfs-datanode
   ```

2. **Erreur de cl√© API**
   ```bash
   # V√©rifier la variable d'environnement
   echo $VISUAL_CROSSING_API_KEY
   
   # Configurer dans Airflow
   # Interface web > Admin > Variables
   ```

3. **Spark non disponible**
   ```bash
   # V√©rifier les logs Spark
   docker-compose logs spark-master
   
   # Red√©marrer Spark
   docker-compose restart spark-master spark-worker
   ```

## üìö Ressources

- [Documentation Visual Crossing API](https://www.visualcrossing.com/resources/documentation/weather-api/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [HDFS User Guide](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
- [Apache Airflow Documentation](https://airflow.apache.org/docs/)

## ü§ù Contribution

Pour contribuer √† ce projet:

1. Fork le repository
2. Cr√©er une branche feature (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit vos changements (`git commit -am 'Ajout nouvelle fonctionnalit√©'`)
4. Push vers la branche (`git push origin feature/nouvelle-fonctionnalite`)
5. Cr√©er une Pull Request
