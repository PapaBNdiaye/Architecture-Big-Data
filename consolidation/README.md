# Architecture Lambda - Big Data

Ce projet implÃ©mente une architecture Lambda complÃ¨te pour le traitement des donnÃ©es mÃ©tÃ©o en temps rÃ©el et en batch.

## ğŸš€ DÃ©marrage Rapide

### Option 1: DÃ©ploiement AutomatisÃ© (RecommandÃ©)

```bash
# ExÃ©cuter le script d'automatisation complet
./deploy_and_run.sh
```

Ce script automatise tout le processus :
- âœ… Configuration de l'API Key Visual Crossing
- âœ… DÃ©marrage des services Docker
- âœ… Installation des dÃ©pendances Python
- âœ… Configuration des permissions HDFS
- âœ… ExÃ©cution du traitement batch
- âœ… VÃ©rification des donnÃ©es

### Option 2: DÃ©ploiement Manuel

```bash
# 1. Configuration de l'API Key
export VISUAL_CROSSING_API_KEY=BYYWRZR3C5BF2TQ3UER8KPTYJ

# 2. DÃ©marrage des services
docker-compose up -d

# 3. Installation des dÃ©pendances
docker exec spark-master pip install -r /opt/spark/requirements_batch.txt
docker exec spark-worker pip install -r /opt/spark/requirements_batch.txt

# 4. Configuration HDFS
docker exec -it hdfs-namenode bash -c "hdfs dfs -mkdir -p /user/spark/weather/raw"
docker exec -it hdfs-namenode bash -c "hdfs dfs -chown -R spark:spark /user/spark"

# 5. ExÃ©cution du traitement batch
docker exec -it spark-master spark-submit \
  --master spark://spark-master:7077 \
  --py-files /opt/spark/project/config/cities_config.py \
  /opt/spark/project/batch_processing_local.py

# 6. VÃ©rification des donnÃ©es
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark/project/count_data.py
```

## ğŸŒ Interfaces Web

Une fois les services dÃ©marrÃ©s, vous pouvez accÃ©der aux interfaces suivantes :

- **HDFS Web UI**: http://localhost:9871
- **Spark Master UI**: http://localhost:9090  
- **Airflow UI**: http://localhost:8080
- **Kafka Control Center**: http://localhost:9021

## ğŸ“Š Visualisation des DonnÃ©es HDFS

### Via l'Interface Web
1. Ouvrez http://localhost:9871
2. Naviguez vers `/user/spark/weather/raw/`
3. Explorez les rÃ©pertoires par timestamp

### Via les Commandes
```bash
# Lister les rÃ©pertoires
docker exec hdfs-namenode hdfs dfs -ls /user/spark/weather/raw/

# Voir le contenu d'un rÃ©pertoire spÃ©cifique
docker exec hdfs-namenode hdfs dfs -ls /user/spark/weather/raw/20250925_101201/

# Compter les donnÃ©es avec le script Python
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark/project/count_data.py
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Visual    â”‚â”€â”€â”€â–¶â”‚  Batch Processor â”‚â”€â”€â”€â–¶â”‚      HDFS      â”‚
â”‚   Crossing      â”‚    â”‚   (Spark)       â”‚    â”‚   (Stockage)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     Airflow     â”‚
                       â”‚  (Orchestration) â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Structure des DonnÃ©es

Les donnÃ©es sont organisÃ©es dans HDFS comme suit :
```
/user/spark/weather/
â”œâ”€â”€ raw/                    # DonnÃ©es brutes de l'API
â”‚   â”œâ”€â”€ 20250925_101201/   # Timestamp d'exÃ©cution
â”‚   â”‚   â”œâ”€â”€ Paris_FR.parquet
â”‚   â”‚   â”œâ”€â”€ Lyon_FR.parquet
â”‚   â”‚   â””â”€â”€ ...
â””â”€â”€ batch/                  # DonnÃ©es agrÃ©gÃ©es (futur)
```

## ğŸ”§ DÃ©pannage

### Services non dÃ©marrÃ©s
```bash
# VÃ©rifier le statut des conteneurs
docker-compose ps

# RedÃ©marrer un service spÃ©cifique
docker-compose restart hdfs-namenode
```

### ProblÃ¨mes de permissions HDFS
```bash
# RÃ©initialiser les permissions
docker exec -it hdfs-namenode bash -c "hdfs dfs -chown -R spark:spark /user/spark"
```

### Logs des services
```bash
# Voir les logs d'un service
docker logs hdfs-namenode
docker logs spark-master
```

## ğŸ“‹ PrÃ©requis

- Docker et Docker Compose
- Python 3.8+
- ClÃ© API Visual Crossing (dÃ©jÃ  configurÃ©e dans le script)

## ğŸ¯ FonctionnalitÃ©s

- **Traitement Batch**: RÃ©cupÃ©ration et traitement des donnÃ©es mÃ©tÃ©o
- **Stockage DistribuÃ©**: HDFS pour la persistance des donnÃ©es
- **Orchestration**: Airflow pour la planification des tÃ¢ches
- **Streaming**: Kafka pour le traitement en temps rÃ©el
- **Monitoring**: Interfaces web pour le suivi des services
